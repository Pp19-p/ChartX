{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 22:55:51.854290: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, input_shape=(299, 299, 3), dropout_rates=[0.3, 0.4, 0.3]):\n",
    "    \"\"\"Create an Xception-based transfer learning model with custom top layers\"\"\"\n",
    "    base_model = applications.Xception(\n",
    "        weights=\"imagenet\", \n",
    "        include_top=False, \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    input_layer = Input(shape=input_shape, name='image_input')\n",
    "    x = base_model(input_layer)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[0])(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[1])(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rates[2])(x)\n",
    "    \n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, fold=None):\n",
    "    \"\"\"Plot training and validation accuracy/loss\"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f'training_history_fold_{fold}.png' if fold is not None else 'training_history.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(train_dir, train_csv, output_dir, n_folds=5, epochs=50):\n",
    "    \"\"\"Perform k-fold cross-validation\"\"\"\n",
    "\n",
    "    traindf = pd.read_csv(train_csv)\n",
    "    traindf['code'] = traindf['type'].astype('category').cat.codes\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(traindf), 1):\n",
    "        print(f'\\nTraining Fold {fold}/{n_folds}')\n",
    "        \n",
    "        train_df = traindf.iloc[train_idx]\n",
    "        val_df = traindf.iloc[val_idx]\n",
    "        \n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory=train_dir,\n",
    "            x_col=\"chart\",\n",
    "            y_col=\"type\",\n",
    "            batch_size=32,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=(299, 299)\n",
    "        )\n",
    "\n",
    "        val_generator = val_datagen.flow_from_dataframe(\n",
    "            dataframe=val_df,\n",
    "            directory=train_dir,\n",
    "            x_col=\"chart\",\n",
    "            y_col=\"type\",\n",
    "            batch_size=32,\n",
    "            seed=42,\n",
    "            shuffle=False,\n",
    "            class_mode=\"categorical\",\n",
    "            target_size=(299, 299)\n",
    "        )\n",
    "\n",
    "        num_classes = len(traindf['type'].unique())\n",
    "        model = create_model(num_classes)\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "            ModelCheckpoint(\n",
    "                os.path.join(output_dir, f'best_model_fold_{fold}.h5'),\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy'\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plot_training_history(history, fold)\n",
    "\n",
    "        # Evaluate\n",
    "        val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "        fold_scores.append({\n",
    "            'fold': fold,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        })\n",
    "        \n",
    "        model_json = model.to_json()\n",
    "        with open(os.path.join(output_dir, f\"model_fold_{fold}.json\"), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(os.path.join(output_dir, f\"model_fold_{fold}.h5\"))\n",
    "\n",
    "    cv_results = pd.DataFrame(fold_scores)\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    print(cv_results)\n",
    "    print(f\"\\nMean validation accuracy: {cv_results['val_accuracy'].mean():.4f}\")\n",
    "    print(f\"Standard deviation: {cv_results['val_accuracy'].std():.4f}\")\n",
    "    \n",
    "    cv_results.to_csv(os.path.join(output_dir, 'cross_validation_results.csv'), index=False)\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, test_dir, test_csv, train_generator, output_dir):\n",
    "    \"\"\"Make predictions on test data and visualize results\"\"\"\n",
    "   \n",
    "    testdf = pd.read_csv(test_csv)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=testdf,\n",
    "        directory=test_dir,\n",
    "        x_col=\"chart\",\n",
    "        y_col=None,\n",
    "        batch_size=1,\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        class_mode=None,\n",
    "        target_size=(299, 299)\n",
    "    )\n",
    "\n",
    "    test_generator.reset()\n",
    "    pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "    \n",
    "    predicted_class_indices = np.argmax(pred, axis=1)\n",
    "    labels = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "    predictions = [labels[k] for k in predicted_class_indices]\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        \"chart\": test_generator.filenames,\n",
    "        \"predicted_type\": predictions,\n",
    "        \"confidence\": np.max(pred, axis=1)\n",
    "    })\n",
    "    results.to_csv(os.path.join(output_dir, \"prediction.csv\"), index=False)\n",
    "\n",
    "    barplots = results[results['predicted_type'] == 'BarPlot']\n",
    "    num_images = min(len(barplots), 100)\n",
    "    \n",
    "    if num_images > 0:\n",
    "        rows = int(np.ceil(np.sqrt(num_images)))\n",
    "        cols = int(np.ceil(num_images / rows))\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "        fig.suptitle('Predicted Bar Plot Samples', fontsize=16)\n",
    "        \n",
    "        for index in range(num_images):\n",
    "            image_path = os.path.join(test_dir, barplots.iloc[index]['chart'])\n",
    "            row = index // cols\n",
    "            col = index % cols\n",
    "            \n",
    "            try:\n",
    "                img = mpimg.imread(image_path)\n",
    "                if rows == 1:\n",
    "                    axes[col].imshow(img)\n",
    "                    axes[col].axis('off')\n",
    "                else:\n",
    "                    axes[row, col].imshow(img)\n",
    "                    axes[row, col].axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'bar_plot_predictions.png'))\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Prediction visualization complete.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/5\n",
      "Found 16197 validated image filenames belonging to 19 classes.\n",
      "Found 4050 validated image filenames belonging to 19 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 22:55:53.720920: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-01 22:55:54.192159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22302 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:53:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 22:55:59.793886: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2025-02-01 22:56:01.404794: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/507 [..............................] - ETA: 3:43 - loss: 3.3814 - accuracy: 0.1500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 275s 532ms/step - loss: 0.6036 - accuracy: 0.8479 - val_loss: 0.2696 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.2741 - accuracy: 0.9254 - val_loss: 0.2012 - val_accuracy: 0.9449 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.2114 - accuracy: 0.9374 - val_loss: 0.1913 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1753 - accuracy: 0.9489 - val_loss: 0.1683 - val_accuracy: 0.9504 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1457 - accuracy: 0.9567 - val_loss: 0.1808 - val_accuracy: 0.9489 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.1219 - accuracy: 0.9616 - val_loss: 0.1736 - val_accuracy: 0.9504 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.1079 - accuracy: 0.9651 - val_loss: 0.1752 - val_accuracy: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0982 - accuracy: 0.9675 - val_loss: 0.1716 - val_accuracy: 0.9560 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0944 - accuracy: 0.9693 - val_loss: 0.1624 - val_accuracy: 0.9543 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0757 - accuracy: 0.9740 - val_loss: 0.1956 - val_accuracy: 0.9543 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0735 - accuracy: 0.9743 - val_loss: 0.1659 - val_accuracy: 0.9565 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0668 - accuracy: 0.9781 - val_loss: 0.2075 - val_accuracy: 0.9553 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0606 - accuracy: 0.9809 - val_loss: 0.2075 - val_accuracy: 0.9501 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "507/507 [==============================] - 266s 524ms/step - loss: 0.0561 - accuracy: 0.9812 - val_loss: 0.1984 - val_accuracy: 0.9580 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0387 - accuracy: 0.9856 - val_loss: 0.1920 - val_accuracy: 0.9558 - lr: 2.0000e-05\n",
      "Epoch 16/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.0330 - accuracy: 0.9883 - val_loss: 0.1915 - val_accuracy: 0.9553 - lr: 2.0000e-05\n",
      "Epoch 17/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.0285 - accuracy: 0.9897 - val_loss: 0.1894 - val_accuracy: 0.9560 - lr: 2.0000e-05\n",
      "Epoch 18/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.1917 - val_accuracy: 0.9583 - lr: 2.0000e-05\n",
      "Epoch 19/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0267 - accuracy: 0.9899 - val_loss: 0.1965 - val_accuracy: 0.9570 - lr: 2.0000e-05\n",
      "127/127 [==============================] - 16s 124ms/step - loss: 0.1624 - accuracy: 0.9543\n",
      "\n",
      "Training Fold 2/5\n",
      "Found 16197 validated image filenames belonging to 19 classes.\n",
      "Found 4050 validated image filenames belonging to 19 classes.\n",
      "Epoch 1/50\n",
      " 67/507 [==>...........................] - ETA: 3:38 - loss: 1.5955 - accuracy: 0.6063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 270s 529ms/step - loss: 0.6041 - accuracy: 0.8492 - val_loss: 0.2574 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.2716 - accuracy: 0.9250 - val_loss: 0.1791 - val_accuracy: 0.9447 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.2115 - accuracy: 0.9353 - val_loss: 0.1592 - val_accuracy: 0.9511 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.1697 - accuracy: 0.9488 - val_loss: 0.1688 - val_accuracy: 0.9541 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.1531 - accuracy: 0.9524 - val_loss: 0.1582 - val_accuracy: 0.9533 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1273 - accuracy: 0.9608 - val_loss: 0.1641 - val_accuracy: 0.9519 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.1217 - accuracy: 0.9614 - val_loss: 0.1526 - val_accuracy: 0.9578 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.1000 - accuracy: 0.9670 - val_loss: 0.1555 - val_accuracy: 0.9543 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0935 - accuracy: 0.9691 - val_loss: 0.1635 - val_accuracy: 0.9588 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.0839 - accuracy: 0.9720 - val_loss: 0.1494 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "507/507 [==============================] - 271s 534ms/step - loss: 0.0746 - accuracy: 0.9745 - val_loss: 0.1558 - val_accuracy: 0.9607 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0754 - accuracy: 0.9751 - val_loss: 0.1663 - val_accuracy: 0.9568 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0620 - accuracy: 0.9786 - val_loss: 0.1572 - val_accuracy: 0.9630 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.0566 - accuracy: 0.9796 - val_loss: 0.1579 - val_accuracy: 0.9580 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "507/507 [==============================] - 271s 534ms/step - loss: 0.0568 - accuracy: 0.9805 - val_loss: 0.1603 - val_accuracy: 0.9644 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0418 - accuracy: 0.9863 - val_loss: 0.1493 - val_accuracy: 0.9640 - lr: 2.0000e-05\n",
      "Epoch 17/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.0322 - accuracy: 0.9884 - val_loss: 0.1469 - val_accuracy: 0.9635 - lr: 2.0000e-05\n",
      "Epoch 18/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 0.1476 - val_accuracy: 0.9642 - lr: 2.0000e-05\n",
      "Epoch 19/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.1515 - val_accuracy: 0.9654 - lr: 2.0000e-05\n",
      "Epoch 20/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.0246 - accuracy: 0.9910 - val_loss: 0.1533 - val_accuracy: 0.9649 - lr: 2.0000e-05\n",
      "Epoch 21/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0253 - accuracy: 0.9901 - val_loss: 0.1530 - val_accuracy: 0.9652 - lr: 2.0000e-05\n",
      "Epoch 22/50\n",
      "507/507 [==============================] - 271s 534ms/step - loss: 0.0236 - accuracy: 0.9916 - val_loss: 0.1570 - val_accuracy: 0.9659 - lr: 2.0000e-05\n",
      "Epoch 23/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 0.1539 - val_accuracy: 0.9657 - lr: 4.0000e-06\n",
      "Epoch 24/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.1541 - val_accuracy: 0.9652 - lr: 4.0000e-06\n",
      "Epoch 25/50\n",
      "507/507 [==============================] - 266s 524ms/step - loss: 0.0197 - accuracy: 0.9926 - val_loss: 0.1559 - val_accuracy: 0.9644 - lr: 4.0000e-06\n",
      "Epoch 26/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.1569 - val_accuracy: 0.9642 - lr: 4.0000e-06\n",
      "Epoch 27/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 0.1572 - val_accuracy: 0.9647 - lr: 4.0000e-06\n",
      "127/127 [==============================] - 17s 132ms/step - loss: 0.1469 - accuracy: 0.9635\n",
      "\n",
      "Training Fold 3/5\n",
      "Found 16198 validated image filenames belonging to 19 classes.\n",
      "Found 4049 validated image filenames belonging to 19 classes.\n",
      "Epoch 1/50\n",
      " 67/507 [==>...........................] - ETA: 3:39 - loss: 1.4910 - accuracy: 0.6241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 272s 533ms/step - loss: 0.5940 - accuracy: 0.8516 - val_loss: 0.2569 - val_accuracy: 0.9321 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.2725 - accuracy: 0.9260 - val_loss: 0.2017 - val_accuracy: 0.9412 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.2126 - accuracy: 0.9377 - val_loss: 0.1613 - val_accuracy: 0.9496 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.1670 - accuracy: 0.9491 - val_loss: 0.1586 - val_accuracy: 0.9489 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1527 - accuracy: 0.9525 - val_loss: 0.1525 - val_accuracy: 0.9521 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1230 - accuracy: 0.9607 - val_loss: 0.1604 - val_accuracy: 0.9511 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.1201 - accuracy: 0.9620 - val_loss: 0.1499 - val_accuracy: 0.9580 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0961 - accuracy: 0.9688 - val_loss: 0.1455 - val_accuracy: 0.9555 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.0863 - accuracy: 0.9710 - val_loss: 0.1538 - val_accuracy: 0.9588 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0830 - accuracy: 0.9735 - val_loss: 0.1571 - val_accuracy: 0.9541 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.0746 - accuracy: 0.9737 - val_loss: 0.1714 - val_accuracy: 0.9543 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0757 - accuracy: 0.9735 - val_loss: 0.1770 - val_accuracy: 0.9575 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0591 - accuracy: 0.9798 - val_loss: 0.1541 - val_accuracy: 0.9605 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0457 - accuracy: 0.9847 - val_loss: 0.1532 - val_accuracy: 0.9592 - lr: 2.0000e-05\n",
      "Epoch 15/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0383 - accuracy: 0.9875 - val_loss: 0.1497 - val_accuracy: 0.9592 - lr: 2.0000e-05\n",
      "Epoch 16/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0400 - accuracy: 0.9867 - val_loss: 0.1541 - val_accuracy: 0.9595 - lr: 2.0000e-05\n",
      "Epoch 17/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0351 - accuracy: 0.9876 - val_loss: 0.1533 - val_accuracy: 0.9605 - lr: 2.0000e-05\n",
      "Epoch 18/50\n",
      "507/507 [==============================] - 265s 523ms/step - loss: 0.0339 - accuracy: 0.9878 - val_loss: 0.1512 - val_accuracy: 0.9605 - lr: 2.0000e-05\n",
      "127/127 [==============================] - 17s 130ms/step - loss: 0.1455 - accuracy: 0.9555\n",
      "\n",
      "Training Fold 4/5\n",
      "Found 16198 validated image filenames belonging to 19 classes.\n",
      "Found 4049 validated image filenames belonging to 19 classes.\n",
      "Epoch 1/50\n",
      " 19/507 [>.............................] - ETA: 4:01 - loss: 2.6868 - accuracy: 0.2780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 272s 531ms/step - loss: 0.6168 - accuracy: 0.8505 - val_loss: 0.2136 - val_accuracy: 0.9380 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.2668 - accuracy: 0.9270 - val_loss: 0.1669 - val_accuracy: 0.9481 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.2142 - accuracy: 0.9392 - val_loss: 0.1547 - val_accuracy: 0.9521 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.1718 - accuracy: 0.9485 - val_loss: 0.1477 - val_accuracy: 0.9551 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.1460 - accuracy: 0.9542 - val_loss: 0.1474 - val_accuracy: 0.9551 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.1222 - accuracy: 0.9605 - val_loss: 0.1515 - val_accuracy: 0.9568 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "507/507 [==============================] - 271s 535ms/step - loss: 0.1193 - accuracy: 0.9624 - val_loss: 0.1445 - val_accuracy: 0.9622 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0972 - accuracy: 0.9678 - val_loss: 0.1360 - val_accuracy: 0.9597 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "507/507 [==============================] - 270s 532ms/step - loss: 0.0957 - accuracy: 0.9682 - val_loss: 0.1449 - val_accuracy: 0.9627 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0841 - accuracy: 0.9718 - val_loss: 0.1297 - val_accuracy: 0.9622 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.0702 - accuracy: 0.9765 - val_loss: 0.1494 - val_accuracy: 0.9610 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0679 - accuracy: 0.9768 - val_loss: 0.1508 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0641 - accuracy: 0.9783 - val_loss: 0.1398 - val_accuracy: 0.9652 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.0604 - accuracy: 0.9786 - val_loss: 0.1545 - val_accuracy: 0.9607 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.0521 - accuracy: 0.9822 - val_loss: 0.1588 - val_accuracy: 0.9560 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "507/507 [==============================] - 264s 520ms/step - loss: 0.0396 - accuracy: 0.9871 - val_loss: 0.1370 - val_accuracy: 0.9667 - lr: 2.0000e-05\n",
      "Epoch 17/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.0312 - accuracy: 0.9886 - val_loss: 0.1366 - val_accuracy: 0.9676 - lr: 2.0000e-05\n",
      "Epoch 18/50\n",
      "507/507 [==============================] - 270s 533ms/step - loss: 0.0282 - accuracy: 0.9899 - val_loss: 0.1398 - val_accuracy: 0.9642 - lr: 2.0000e-05\n",
      "Epoch 19/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0267 - accuracy: 0.9902 - val_loss: 0.1369 - val_accuracy: 0.9659 - lr: 2.0000e-05\n",
      "Epoch 20/50\n",
      "507/507 [==============================] - 266s 525ms/step - loss: 0.0259 - accuracy: 0.9907 - val_loss: 0.1421 - val_accuracy: 0.9659 - lr: 2.0000e-05\n",
      "127/127 [==============================] - 17s 130ms/step - loss: 0.1297 - accuracy: 0.9622\n",
      "\n",
      "Training Fold 5/5\n",
      "Found 16198 validated image filenames belonging to 19 classes.\n",
      "Found 4049 validated image filenames belonging to 19 classes.\n",
      "Epoch 1/50\n",
      " 16/507 [..............................] - ETA: 3:59 - loss: 2.9041 - accuracy: 0.2363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 272s 532ms/step - loss: 0.6167 - accuracy: 0.8461 - val_loss: 0.2615 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "507/507 [==============================] - 266s 525ms/step - loss: 0.2694 - accuracy: 0.9241 - val_loss: 0.1970 - val_accuracy: 0.9439 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.2077 - accuracy: 0.9383 - val_loss: 0.1753 - val_accuracy: 0.9474 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.1717 - accuracy: 0.9479 - val_loss: 0.1676 - val_accuracy: 0.9553 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "507/507 [==============================] - 269s 530ms/step - loss: 0.1461 - accuracy: 0.9559 - val_loss: 0.1645 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.1241 - accuracy: 0.9602 - val_loss: 0.1707 - val_accuracy: 0.9531 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.1156 - accuracy: 0.9617 - val_loss: 0.1621 - val_accuracy: 0.9560 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0957 - accuracy: 0.9673 - val_loss: 0.1755 - val_accuracy: 0.9543 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "507/507 [==============================] - 266s 524ms/step - loss: 0.0916 - accuracy: 0.9681 - val_loss: 0.1607 - val_accuracy: 0.9551 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.0857 - accuracy: 0.9709 - val_loss: 0.1804 - val_accuracy: 0.9516 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0759 - accuracy: 0.9746 - val_loss: 0.1850 - val_accuracy: 0.9585 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.1691 - val_accuracy: 0.9570 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "507/507 [==============================] - 268s 529ms/step - loss: 0.0660 - accuracy: 0.9765 - val_loss: 0.1720 - val_accuracy: 0.9592 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "507/507 [==============================] - 271s 533ms/step - loss: 0.0563 - accuracy: 0.9797 - val_loss: 0.1913 - val_accuracy: 0.9573 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0447 - accuracy: 0.9847 - val_loss: 0.1649 - val_accuracy: 0.9615 - lr: 2.0000e-05\n",
      "Epoch 16/50\n",
      "507/507 [==============================] - 267s 526ms/step - loss: 0.0367 - accuracy: 0.9870 - val_loss: 0.1618 - val_accuracy: 0.9622 - lr: 2.0000e-05\n",
      "Epoch 17/50\n",
      "507/507 [==============================] - 269s 531ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.1619 - val_accuracy: 0.9620 - lr: 2.0000e-05\n",
      "Epoch 18/50\n",
      "507/507 [==============================] - 267s 527ms/step - loss: 0.0301 - accuracy: 0.9886 - val_loss: 0.1679 - val_accuracy: 0.9605 - lr: 2.0000e-05\n",
      "Epoch 19/50\n",
      "507/507 [==============================] - 268s 528ms/step - loss: 0.0272 - accuracy: 0.9907 - val_loss: 0.1676 - val_accuracy: 0.9627 - lr: 2.0000e-05\n",
      "127/127 [==============================] - 17s 132ms/step - loss: 0.1607 - accuracy: 0.9551\n",
      "\n",
      "Cross-validation results:\n",
      "   fold  val_loss  val_accuracy\n",
      "0     1  0.162410      0.954321\n",
      "1     2  0.146916      0.963457\n",
      "2     3  0.145489      0.955545\n",
      "3     4  0.129693      0.962213\n",
      "4     5  0.160661      0.955051\n",
      "\n",
      "Mean validation accuracy: 0.9581\n",
      "Standard deviation: 0.0044\n",
      "Found 20247 validated image filenames belonging to 19 classes.\n",
      "Epoch 1/47\n",
      "  5/633 [..............................] - ETA: 5:11 - loss: 3.7084 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633/633 [==============================] - 315s 493ms/step - loss: 0.5331 - accuracy: 0.8645 - lr: 1.0000e-04\n",
      "Epoch 2/47\n",
      "633/633 [==============================] - 317s 500ms/step - loss: 0.2518 - accuracy: 0.9299 - lr: 1.0000e-04\n",
      "Epoch 3/47\n",
      "633/633 [==============================] - 316s 499ms/step - loss: 0.1986 - accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 4/47\n",
      "633/633 [==============================] - 316s 499ms/step - loss: 0.1598 - accuracy: 0.9505 - lr: 1.0000e-04\n",
      "Epoch 5/47\n",
      "633/633 [==============================] - 313s 494ms/step - loss: 0.1353 - accuracy: 0.9564 - lr: 1.0000e-04\n",
      "Epoch 6/47\n",
      "633/633 [==============================] - 314s 497ms/step - loss: 0.1195 - accuracy: 0.9604 - lr: 1.0000e-04\n",
      "Epoch 7/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.1058 - accuracy: 0.9653 - lr: 1.0000e-04\n",
      "Epoch 8/47\n",
      "633/633 [==============================] - 314s 495ms/step - loss: 0.0981 - accuracy: 0.9671 - lr: 1.0000e-04\n",
      "Epoch 9/47\n",
      "633/633 [==============================] - 316s 499ms/step - loss: 0.0825 - accuracy: 0.9719 - lr: 1.0000e-04\n",
      "Epoch 10/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.0732 - accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 11/47\n",
      "633/633 [==============================] - 318s 502ms/step - loss: 0.0659 - accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 12/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.0646 - accuracy: 0.9780 - lr: 1.0000e-04\n",
      "Epoch 13/47\n",
      "633/633 [==============================] - 312s 493ms/step - loss: 0.0570 - accuracy: 0.9802 - lr: 1.0000e-04\n",
      "Epoch 14/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0547 - accuracy: 0.9807 - lr: 1.0000e-04\n",
      "Epoch 15/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0478 - accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 16/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0444 - accuracy: 0.9843 - lr: 1.0000e-04\n",
      "Epoch 17/47\n",
      "633/633 [==============================] - 312s 493ms/step - loss: 0.0392 - accuracy: 0.9853 - lr: 1.0000e-04\n",
      "Epoch 18/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.0423 - accuracy: 0.9848 - lr: 1.0000e-04\n",
      "Epoch 19/47\n",
      "633/633 [==============================] - 315s 498ms/step - loss: 0.0376 - accuracy: 0.9871 - lr: 1.0000e-04\n",
      "Epoch 20/47\n",
      "633/633 [==============================] - 316s 499ms/step - loss: 0.0361 - accuracy: 0.9886 - lr: 1.0000e-04\n",
      "Epoch 21/47\n",
      "633/633 [==============================] - 312s 494ms/step - loss: 0.0321 - accuracy: 0.9878 - lr: 1.0000e-04\n",
      "Epoch 22/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0339 - accuracy: 0.9886 - lr: 1.0000e-04\n",
      "Epoch 23/47\n",
      "633/633 [==============================] - 313s 495ms/step - loss: 0.0312 - accuracy: 0.9895 - lr: 1.0000e-04\n",
      "Epoch 24/47\n",
      "633/633 [==============================] - 322s 508ms/step - loss: 0.0264 - accuracy: 0.9909 - lr: 1.0000e-04\n",
      "Epoch 25/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0305 - accuracy: 0.9899 - lr: 1.0000e-04\n",
      "Epoch 26/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0284 - accuracy: 0.9902 - lr: 1.0000e-04\n",
      "Epoch 27/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0216 - accuracy: 0.9928 - lr: 1.0000e-04\n",
      "Epoch 28/47\n",
      "633/633 [==============================] - 317s 501ms/step - loss: 0.0207 - accuracy: 0.9923 - lr: 1.0000e-04\n",
      "Epoch 29/47\n",
      "633/633 [==============================] - 315s 498ms/step - loss: 0.0255 - accuracy: 0.9921 - lr: 1.0000e-04\n",
      "Epoch 30/47\n",
      "633/633 [==============================] - 312s 493ms/step - loss: 0.0251 - accuracy: 0.9917 - lr: 1.0000e-04\n",
      "Epoch 31/47\n",
      "633/633 [==============================] - 315s 498ms/step - loss: 0.0240 - accuracy: 0.9924 - lr: 1.0000e-04\n",
      "Epoch 32/47\n",
      "633/633 [==============================] - 312s 492ms/step - loss: 0.0222 - accuracy: 0.9927 - lr: 1.0000e-04\n",
      "Epoch 33/47\n",
      "633/633 [==============================] - 313s 494ms/step - loss: 0.0180 - accuracy: 0.9942 - lr: 1.0000e-04\n",
      "Epoch 34/47\n",
      "633/633 [==============================] - 312s 493ms/step - loss: 0.0202 - accuracy: 0.9929 - lr: 1.0000e-04\n",
      "Epoch 35/47\n",
      "633/633 [==============================] - 313s 495ms/step - loss: 0.0214 - accuracy: 0.9923 - lr: 1.0000e-04\n",
      "Epoch 36/47\n",
      "633/633 [==============================] - 313s 495ms/step - loss: 0.0178 - accuracy: 0.9938 - lr: 1.0000e-04\n",
      "Epoch 37/47\n",
      "633/633 [==============================] - 312s 493ms/step - loss: 0.0185 - accuracy: 0.9943 - lr: 1.0000e-04\n",
      "Epoch 38/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.0149 - accuracy: 0.9953 - lr: 1.0000e-04\n",
      "Epoch 39/47\n",
      "633/633 [==============================] - 311s 491ms/step - loss: 0.0192 - accuracy: 0.9939 - lr: 1.0000e-04\n",
      "Epoch 40/47\n",
      "633/633 [==============================] - 316s 500ms/step - loss: 0.0141 - accuracy: 0.9954 - lr: 1.0000e-04\n",
      "Epoch 41/47\n",
      "633/633 [==============================] - 315s 498ms/step - loss: 0.0154 - accuracy: 0.9953 - lr: 1.0000e-04\n",
      "Epoch 42/47\n",
      "633/633 [==============================] - 315s 498ms/step - loss: 0.0183 - accuracy: 0.9943 - lr: 1.0000e-04\n",
      "Epoch 43/47\n",
      "633/633 [==============================] - 314s 496ms/step - loss: 0.0172 - accuracy: 0.9946 - lr: 1.0000e-04\n",
      "Epoch 44/47\n",
      "633/633 [==============================] - 315s 497ms/step - loss: 0.0152 - accuracy: 0.9950 - lr: 1.0000e-04\n",
      "Epoch 45/47\n",
      "633/633 [==============================] - 313s 494ms/step - loss: 0.0141 - accuracy: 0.9952 - lr: 1.0000e-04\n",
      "Epoch 46/47\n",
      "633/633 [==============================] - 318s 502ms/step - loss: 0.0097 - accuracy: 0.9966 - lr: 2.0000e-05\n",
      "Epoch 47/47\n",
      "633/633 [==============================] - 313s 494ms/step - loss: 0.0065 - accuracy: 0.9978 - lr: 2.0000e-05\n",
      "Found 140 validated image filenames.\n",
      " 87/140 [=================>............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 2s 9ms/step\n",
      "Prediction visualization complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    TRAIN_DIR = \"/root/autodl-tmp/Dataset/Train\"\n",
    "    TRAIN_CSV = \"/root/autodl-tmp/Dataset/train.csv\"\n",
    "    TEST_DIR = \"/root/autodl-tmp/Dataset/Test\"\n",
    "    TEST_CSV = \"/root/autodl-tmp/Dataset/test.csv\"\n",
    "    OUTPUT_DIR = \"/root/autodl-tmp/Dataset/model_output\"\n",
    "    \n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    cv_results = cross_validate_model(TRAIN_DIR, TRAIN_CSV, OUTPUT_DIR, n_folds=5, epochs=50)\n",
    "    \n",
    "    traindf = pd.read_csv(TRAIN_CSV)\n",
    "    num_classes = len(traindf['type'].unique())\n",
    "    final_model = create_model(num_classes)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255.,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=traindf,\n",
    "        directory=TRAIN_DIR,\n",
    "        x_col=\"chart\",\n",
    "        y_col=\"type\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(299, 299)\n",
    "    )\n",
    "    \n",
    "    final_model.fit(\n",
    "        train_generator,\n",
    "        epochs=int(cv_results['val_accuracy'].mean() * 50),  # Adjust epochs based on CV performance\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "            ModelCheckpoint(\n",
    "                os.path.join(OUTPUT_DIR, 'model.h5'),\n",
    "                save_best_only=True,\n",
    "                monitor='loss'\n",
    "            )\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_json = final_model.to_json()\n",
    "    with open(os.path.join(OUTPUT_DIR, \"model.json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    final_model.save_weights(os.path.join(OUTPUT_DIR, \"model.h5\"))\n",
    "    \n",
    "    predictions = predict_and_visualize(\n",
    "        final_model,\n",
    "        TEST_DIR,\n",
    "        TEST_CSV,\n",
    "        train_generator,\n",
    "        OUTPUT_DIR\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
